{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_loader import load_sample_data\n",
    "from evaluate import evaluate\n",
    "from inference import generate_text\n",
    "from tokenizer import tokenize_text\n",
    "from train import create_batches, train_model\n",
    "from transformer_model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    config = ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "\n",
    "    hyperparameters = config[\"Hyperparameters\"]\n",
    "    num_samples = int(hyperparameters[\"num_samples\"])\n",
    "    batch_size = int(hyperparameters[\"batch_size\"])\n",
    "    seq_length = int(hyperparameters[\"seq_length\"])\n",
    "    num_epochs = int(hyperparameters[\"num_epochs\"])\n",
    "    learning_rate = float(hyperparameters[\"learning_rate\"])\n",
    "    scheduler_patience = int(hyperparameters[\"scheduler_patience\"])\n",
    "    scheduler_factor = float(hyperparameters[\"scheduler_factor\"])\n",
    "    max_vocab_size = int(hyperparameters[\"max_vocab_size\"])\n",
    "    embedding_dim = int(hyperparameters[\"embedding_dim\"])\n",
    "    ff_hidden_dim = int(hyperparameters[\"ff_hidden_dim\"])\n",
    "    num_blocks = int(hyperparameters[\"num_blocks\"])\n",
    "    initial_text = hyperparameters[\"initial_text\"]\n",
    "    max_len = int(hyperparameters[\"max_len\"])\n",
    "    temperature = float(hyperparameters[\"temperature\"])\n",
    "\n",
    "    return (\n",
    "        num_samples,\n",
    "        batch_size,\n",
    "        seq_length,\n",
    "        num_epochs,\n",
    "        learning_rate,\n",
    "        scheduler_patience,\n",
    "        scheduler_factor,\n",
    "        max_vocab_size,\n",
    "        embedding_dim,\n",
    "        ff_hidden_dim,\n",
    "        num_blocks,\n",
    "        initial_text,\n",
    "        max_len,\n",
    "        temperature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load hyperparameters\n",
    "(\n",
    "    num_samples,\n",
    "    batch_size,\n",
    "    seq_length,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    max_vocab_size,\n",
    "    embedding_dim,\n",
    "    ff_hidden_dim,\n",
    "    num_blocks,\n",
    "    initial_text,\n",
    "    max_len,\n",
    "    temperature,\n",
    ") = get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./wikipedia-20230701\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and pre-process data\n",
    "sample_text = load_sample_data(num_samples=num_samples)\n",
    "vocab, word_to_idx, idx_to_word = tokenize_text(sample_text)\n",
    "tokens = sample_text.split()\n",
    "\n",
    "if len(tokens) > max_vocab_size:\n",
    "    # Count the frequency of each word in your corpus\n",
    "    word_freqs = Counter(tokens)\n",
    "\n",
    "    # Get the most common words up to MAX_VOCAB_SIZE\n",
    "    vocab = [word for word, freq in word_freqs.most_common(max_vocab_size - 1)]\n",
    "\n",
    "    # Add the special <UNK> token to the vocabulary\n",
    "    vocab.append(\"<UNK>\")\n",
    "\n",
    "    # Create word_to_idx dictionary\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # Replace all words not in the vocabulary with <UNK>\n",
    "    tokens = [word if word in word_to_idx else \"<UNK>\" for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create batches\n",
    "input_batches, target_batches = create_batches(\n",
    "    tokens, word_to_idx, batch_size=batch_size, seq_length=seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model...\n"
     ]
    }
   ],
   "source": [
    "# 4. Initialize or load model\n",
    "model_path = \"model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing model...\")\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    print(\"Initializing new model...\")\n",
    "    model = TransformerModel(\n",
    "        vocab_size=max_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        ff_hidden_dim=ff_hidden_dim,\n",
    "        num_blocks=num_blocks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/39..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/slamphear/Developer/lllm/testing.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 5. Train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     vocab,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     num_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     scheduler_patience,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     scheduler_factor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     input_batches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     target_batches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving model...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/lllm/train.py:72\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, vocab, num_epochs, learning_rate, scheduler_patience, scheduler_factor, input_batches, target_batches)\u001b[0m\n\u001b[1;32m     69\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(vocab)), target_batches[i]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     71\u001b[0m \u001b[39m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     73\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     75\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reload hyperparameters before training (in case they've changed)\n",
    "(\n",
    "    num_samples,\n",
    "    batch_size,\n",
    "    seq_length,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    max_vocab_size,\n",
    "    embedding_dim,\n",
    "    ff_hidden_dim,\n",
    "    num_blocks,\n",
    "    initial_text,\n",
    "    max_len,\n",
    "    temperature,\n",
    ") = get_hyperparameters()\n",
    "\n",
    "# 5. Train the model\n",
    "print(\"Training model...\")\n",
    "train_model(\n",
    "    model,\n",
    "    vocab,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    input_batches,\n",
    "    target_batches,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving model...\")\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.8123218504774934\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the model\n",
    "perplexity = evaluate(\n",
    "    model,\n",
    "    input_batches,\n",
    "    target_batches,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    vocab_size=max_vocab_size,\n",
    ")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: There de de de Peso Peso de Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso\n"
     ]
    }
   ],
   "source": [
    "# 7. Generate text\n",
    "generated_text = generate_text(\n",
    "    model,\n",
    "    idx_to_word,\n",
    "    word_to_idx,\n",
    "    initial_text=initial_text,\n",
    "    max_len=max_len,\n",
    "    temperature=temperature,\n",
    ")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

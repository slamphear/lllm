{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_loader import load_sample_data\n",
    "from evaluate import evaluate\n",
    "from inference import generate_text\n",
    "from tokenizer import tokenize_text\n",
    "from train import create_batches, train_model\n",
    "from transformer_model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    config = ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "\n",
    "    hyperparameters = config[\"Hyperparameters\"]\n",
    "    number_of_samples = int(hyperparameters[\"number_of_samples\"])\n",
    "    batch_size = int(hyperparameters[\"batch_size\"])\n",
    "    seq_length = int(hyperparameters[\"seq_length\"])\n",
    "    num_epochs = int(hyperparameters[\"num_epochs\"])\n",
    "    learning_rate = float(hyperparameters[\"learning_rate\"])\n",
    "    scheduler_patience = int(hyperparameters[\"scheduler_patience\"])\n",
    "    scheduler_factor = float(hyperparameters[\"scheduler_factor\"])\n",
    "    max_vocab_size = int(hyperparameters[\"max_vocab_size\"])\n",
    "    embedding_dim = int(hyperparameters[\"embedding_dim\"])\n",
    "    ff_hidden_dim = int(hyperparameters[\"ff_hidden_dim\"])\n",
    "    num_blocks = int(hyperparameters[\"num_blocks\"])\n",
    "    initial_text = hyperparameters[\"initial_text\"]\n",
    "    max_length = int(hyperparameters[\"max_length\"])\n",
    "    temperature = float(hyperparameters[\"temperature\"])\n",
    "\n",
    "    return (\n",
    "        number_of_samples,\n",
    "        batch_size,\n",
    "        seq_length,\n",
    "        num_epochs,\n",
    "        learning_rate,\n",
    "        scheduler_patience,\n",
    "        scheduler_factor,\n",
    "        max_vocab_size,\n",
    "        embedding_dim,\n",
    "        ff_hidden_dim,\n",
    "        num_blocks,\n",
    "        initial_text,\n",
    "        max_length,\n",
    "        temperature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load hyperparameters\n",
    "(\n",
    "    number_of_samples,\n",
    "    batch_size,\n",
    "    seq_length,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    max_vocab_size,\n",
    "    embedding_dim,\n",
    "    ff_hidden_dim,\n",
    "    num_blocks,\n",
    "    initial_text,\n",
    "    max_length,\n",
    "    temperature,\n",
    ") = get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./wikipedia-20230701\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and pre-process data\n",
    "sample_text = load_sample_data(number_of_samples=number_of_samples)\n",
    "vocab, word_to_idx, idx_to_word = tokenize_text(sample_text)\n",
    "tokens = sample_text.split()\n",
    "\n",
    "if len(tokens) > max_vocab_size:\n",
    "    # Count the frequency of each word in your corpus\n",
    "    word_freqs = Counter(tokens)\n",
    "\n",
    "    # Get the most common words up to MAX_VOCAB_SIZE\n",
    "    vocab = [word for word, freq in word_freqs.most_common(max_vocab_size - 1)]\n",
    "\n",
    "    # Add the special <UNK> token to the vocabulary\n",
    "    vocab.append(\"<UNK>\")\n",
    "\n",
    "    # Create word_to_idx dictionary\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    # Replace all words not in the vocabulary with <UNK>\n",
    "    tokens = [word if word in word_to_idx else \"<UNK>\" for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create batches\n",
    "input_batches, target_batches = create_batches(\n",
    "    tokens, word_to_idx, batch_size=batch_size, seq_length=seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model...\n"
     ]
    }
   ],
   "source": [
    "# 4. Initialize or load model\n",
    "model_path = \"model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing model...\")\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    print(\"Initializing new model...\")\n",
    "    model = TransformerModel(\n",
    "        vocab_size=max_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        ff_hidden_dim=ff_hidden_dim,\n",
    "        num_blocks=num_blocks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 1/10, Loss: 4.128260552883148, Learning rate: 0.001\n",
      "Starting epoch 2/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 2/10, Loss: 3.987725257873535, Learning rate: 0.001\n",
      "Starting epoch 3/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 3/10, Loss: 3.5835296511650085, Learning rate: 0.001\n",
      "Starting epoch 4/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 4/10, Loss: 3.2949923872947693, Learning rate: 0.001\n",
      "Starting epoch 5/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 5/10, Loss: 3.002322554588318, Learning rate: 0.001\n",
      "Starting epoch 6/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 6/10, Loss: 2.7230992317199707, Learning rate: 0.001\n",
      "Starting epoch 7/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 7/10, Loss: 2.490257203578949, Learning rate: 0.001\n",
      "Starting epoch 8/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 8/10, Loss: 2.271542191505432, Learning rate: 0.001\n",
      "Starting epoch 9/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 9/10, Loss: 2.073909342288971, Learning rate: 0.001\n",
      "Starting epoch 10/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 10/10, Loss: 1.9030575454235077, Learning rate: 0.001\n",
      "Saving model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 1/10, Loss: 2.2853560149669647, Learning rate: 0.001\n",
      "Starting epoch 2/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 2/10, Loss: 2.3830310702323914, Learning rate: 0.001\n",
      "Starting epoch 3/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 3/10, Loss: 2.047710567712784, Learning rate: 0.001\n",
      "Starting epoch 4/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 4/10, Loss: 1.8192502856254578, Learning rate: 0.001\n",
      "Starting epoch 5/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 5/10, Loss: 1.6250925958156586, Learning rate: 0.001\n",
      "Starting epoch 6/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 6/10, Loss: 1.4671616852283478, Learning rate: 0.001\n",
      "Starting epoch 7/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 7/10, Loss: 1.3331249952316284, Learning rate: 0.001\n",
      "Starting epoch 8/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 8/10, Loss: 1.2191540896892548, Learning rate: 0.001\n",
      "Starting epoch 9/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 9/10, Loss: 1.1206939369440079, Learning rate: 0.001\n",
      "Starting epoch 10/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 10/10, Loss: 1.0381156653165817, Learning rate: 0.001\n",
      "Saving model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 1/10, Loss: 1.3775354772806168, Learning rate: 0.001\n",
      "Starting epoch 2/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 2/10, Loss: 1.5882907211780548, Learning rate: 0.001\n",
      "Starting epoch 3/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 3/10, Loss: 1.3437682390213013, Learning rate: 0.001\n",
      "Starting epoch 4/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 4/10, Loss: 1.2023349404335022, Learning rate: 0.001\n",
      "Starting epoch 5/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 5/10, Loss: 1.0649353712797165, Learning rate: 0.001\n",
      "Starting epoch 6/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 6/10, Loss: 0.9651457518339157, Learning rate: 0.001\n",
      "Starting epoch 7/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 7/10, Loss: 0.8874835073947906, Learning rate: 0.001\n",
      "Starting epoch 8/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 8/10, Loss: 0.8269701153039932, Learning rate: 0.001\n",
      "Starting epoch 9/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 9/10, Loss: 0.7800204455852509, Learning rate: 0.001\n",
      "Starting epoch 10/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 10/10, Loss: 0.7405237704515457, Learning rate: 0.001\n",
      "Saving model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 1/10, Loss: 0.9153897613286972, Learning rate: 0.001\n",
      "Starting epoch 2/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 2/10, Loss: 1.2080256938934326, Learning rate: 0.001\n",
      "Starting epoch 3/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 3/10, Loss: 1.0359387397766113, Learning rate: 0.001\n",
      "Starting epoch 4/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 4/10, Loss: 0.9090117961168289, Learning rate: 0.001\n",
      "Starting epoch 5/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 5/10, Loss: 0.8343984931707382, Learning rate: 0.001\n",
      "Starting epoch 6/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 6/10, Loss: 0.7665172517299652, Learning rate: 0.001\n",
      "Starting epoch 7/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 7/10, Loss: 0.7176748663187027, Learning rate: 0.001\n",
      "Starting epoch 8/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 8/10, Loss: 0.6803742498159409, Learning rate: 0.001\n",
      "Starting epoch 9/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 9/10, Loss: 0.6538485586643219, Learning rate: 0.001\n",
      "Starting epoch 10/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 10/10, Loss: 0.6342554688453674, Learning rate: 0.001\n",
      "Saving model...\n",
      "Starting epoch 1/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 1/10, Loss: 0.7347328066825867, Learning rate: 0.001\n",
      "Starting epoch 2/10:\n",
      "Batch 1/4...2/4...3/4...4/4...Epoch 2/10, Loss: 1.1219115555286407, Learning rate: 0.001\n",
      "Starting epoch 3/10:\n",
      "Batch 1/4...2/4...3/4..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/slamphear/Developer/lllm/testing.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining model...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         vocab,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         num_epochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         learning_rate,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         scheduler_patience,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         scheduler_factor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         input_batches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         target_batches,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/slamphear/Developer/lllm/testing.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving model...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/lllm/train.py:55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, vocab, num_epochs, learning_rate, scheduler_patience, scheduler_factor, input_batches, target_batches)\u001b[0m\n\u001b[1;32m     50\u001b[0m scheduler \u001b[39m=\u001b[39m ReduceLROnPlateau(\n\u001b[1;32m     51\u001b[0m     optimizer, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, patience\u001b[39m=\u001b[39mscheduler_patience, factor\u001b[39m=\u001b[39mscheduler_factor\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBatch \u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# To keep track of loss in an epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/transformer_model.py:24\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Pass through each transformer block\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Output layer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x)\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/transformer_block.py:20\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# Apply attention\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     attention_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m attention_out)\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Apply feedforward network\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/attention.py:23\u001b[0m, in \u001b[0;36mSingleHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_layer(x)\n\u001b[1;32m     22\u001b[0m K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_layer(x)\n\u001b[0;32m---> 23\u001b[0m V \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_layer(x)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Calculate attention scores: (batch_size, seq_length, seq_length)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m attention_scores \u001b[39m=\u001b[39m (\n\u001b[1;32m     27\u001b[0m     torch\u001b[39m.\u001b[39mmatmul(Q, K\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_dim\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/lllm/.direnv/python-3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reload hyperparameters before training (in case they've changed)\n",
    "(\n",
    "    number_of_samples,\n",
    "    batch_size,\n",
    "    seq_length,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    max_vocab_size,\n",
    "    embedding_dim,\n",
    "    ff_hidden_dim,\n",
    "    num_blocks,\n",
    "    initial_text,\n",
    "    max_length,\n",
    "    temperature,\n",
    ") = get_hyperparameters()\n",
    "\n",
    "# 5. Train the model\n",
    "print(\"Training model...\")\n",
    "train_model(\n",
    "    model,\n",
    "    vocab,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    scheduler_patience,\n",
    "    scheduler_factor,\n",
    "    input_batches,\n",
    "    target_batches,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving model...\")\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.412450725474789\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate the model\n",
    "perplexity = evaluate(\n",
    "    model,\n",
    "    input_batches,\n",
    "    target_batches,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    vocab_size=max_vocab_size,\n",
    ")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: There de de de Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso Peso\n"
     ]
    }
   ],
   "source": [
    "# 7. Generate text\n",
    "generated_text = generate_text(\n",
    "    model,\n",
    "    idx_to_word,\n",
    "    word_to_idx,\n",
    "    initial_text=initial_text,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    ")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
